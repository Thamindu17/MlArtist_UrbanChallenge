{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5010eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "ANON = ROOT / \"data\" / \"interim\" / \"anonymized\"\n",
    "\n",
    "# Sub → Main mapping\n",
    "SUB_TO_MAIN = {\n",
    "    \"Pothole\": \"Road issues\",\n",
    "    \"Damaged Road Surface\": \"Road issues\",\n",
    "    \"Illegal Parking\": \"Road issues\",\n",
    "    \"Broken/Damaged Road Sign\": \"Road issues\",\n",
    "    \"Littering / Garbage\": \"Environmental issues\",\n",
    "    \"Vandalism / Graffiti\": \"Environmental issues\",\n",
    "}\n",
    "\n",
    "# Your sub label space (same names we used before)\n",
    "SUB_CLASSES = list(SUB_TO_MAIN.keys())\n",
    "MAIN_CLASSES = sorted(set(SUB_TO_MAIN.values()))\n",
    "MAIN_TO_ID = {m:i for i,m in enumerate(MAIN_CLASSES)}\n",
    "SUB_TO_ID  = {s:i for i,s in enumerate(SUB_CLASSES)}\n",
    "\n",
    "# Where the three auto-labeled sources live (edit if needed)\n",
    "TACO_DIR     = ANON / \"taco\" / \"images\"                 # → Littering / Garbage\n",
    "POTHOLE_DIR  = ANON / \"kaggle_potholes\" / \"images\"      # → Pothole\n",
    "PARKING_DIR  = ANON / \"roboflow_illegal_parking\" / \"images\"  # → Illegal Parking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d32a976d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(833, 665, 103)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "def df_single_label(folder: Path, sub_label: str) -> pd.DataFrame:\n",
    "    exts = (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.webp\",\"*.bmp\",\"*.tiff\")\n",
    "    files = []\n",
    "    for e in exts:\n",
    "        files += glob.glob(str(folder / \"**\" / e), recursive=True)\n",
    "    rows = []\n",
    "    for f in files:\n",
    "        sub = sub_label\n",
    "        main = SUB_TO_MAIN[sub]\n",
    "        rows.append({\n",
    "            \"path\": f,\n",
    "            \"sub_name\": sub,\n",
    "            \"sub_id\":  SUB_TO_ID[sub],\n",
    "            \"main_name\": main,\n",
    "            \"main_id\":  MAIN_TO_ID[main],\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df_taco    = df_single_label(TACO_DIR,    \"Littering / Garbage\")\n",
    "df_pothole = df_single_label(POTHOLE_DIR, \"Pothole\")\n",
    "df_parking = df_single_label(PARKING_DIR, \"Illegal Parking\")\n",
    "\n",
    "len(df_taco), len(df_pothole), len(df_parking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da5da340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, os\n",
    "from urllib.parse import urlparse, parse_qs, unquote\n",
    "\n",
    "def from_ls_url(p: str) -> str:\n",
    "    s = str(p)\n",
    "    if \"/data/local-files/\" in s:\n",
    "        u = urlparse(s); d = parse_qs(u.query).get(\"d\", [None])[0]\n",
    "        if d:\n",
    "            # d might be relative; make absolute under data/interim/\n",
    "            pth = Path(d)\n",
    "            if not pth.is_absolute():\n",
    "                pth = ROOT / \"data\" / \"interim\" / d\n",
    "            return str(pth)\n",
    "    return s\n",
    "\n",
    "def rows_from_ls_export(path: Path):\n",
    "    tasks = json.loads(path.read_text())\n",
    "    rows = []\n",
    "    for t in tasks:\n",
    "        img = (t.get(\"data\") or {}).get(\"image\")\n",
    "        img = from_ls_url(img)\n",
    "        # collect chosen sub labels (Choices)\n",
    "        chosen = set()\n",
    "        for ann in t.get(\"annotations\", []):\n",
    "            for r in ann.get(\"result\", []):\n",
    "                val = r.get(\"value\", {})\n",
    "                for v in (val.get(\"choices\", []) if isinstance(val, dict) else []):\n",
    "                    if v in SUB_TO_MAIN:  # only known subs\n",
    "                        chosen.add(v)\n",
    "        if img and len(chosen)==1:\n",
    "            sub = list(chosen)[0]\n",
    "            main = SUB_TO_MAIN[sub]\n",
    "            rows.append({\n",
    "                \"path\": img,\n",
    "                \"sub_name\": sub,\n",
    "                \"sub_id\":  SUB_TO_ID[sub],\n",
    "                \"main_name\": main,\n",
    "                \"main_id\":  MAIN_TO_ID[main],\n",
    "            })\n",
    "        # NOTE: for images with multiple subs, we skip here to keep single-sub training consistent\n",
    "    return rows\n",
    "\n",
    "ls_exports = sorted((ROOT/\"labels\").glob(\"export*.json\"))\n",
    "rows = []\n",
    "for p in ls_exports:\n",
    "    rows += rows_from_ls_export(p)\n",
    "\n",
    "df_ls = pd.DataFrame(rows, columns=[\"path\",\"sub_name\",\"sub_id\",\"main_name\",\"main_id\"]) if rows else \\\n",
    "        pd.DataFrame(columns=[\"path\",\"sub_name\",\"sub_id\",\"main_name\",\"main_id\"])\n",
    "\n",
    "len(df_ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2998a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1601 rows → /Users/nithilathawalampitiya/Documents/Projects/urban-issue-classifier/labels/dataset_main_sub.csv\n",
      "Main counts: {'Environmental issues': 833, 'Road issues': 768}\n",
      "Sub counts : {'Littering / Garbage': 833, 'Pothole': 643, 'Illegal Parking': 103, 'Damaged Road Surface': 22}\n"
     ]
    }
   ],
   "source": [
    "df_auto = pd.concat([df_taco, df_pothole, df_parking], ignore_index=True)\n",
    "df_all  = pd.concat([df_auto, df_ls], ignore_index=True)\n",
    "df_all  = df_all.drop_duplicates(subset=[\"path\"], keep=\"last\").reset_index(drop=True)\n",
    "\n",
    "OUT = ROOT / \"labels\" / \"dataset_main_sub.csv\"\n",
    "OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_all.to_csv(OUT, index=False)\n",
    "\n",
    "print(\"Wrote\", len(df_all), \"rows →\", OUT)\n",
    "print(\"Main counts:\", df_all[\"main_name\"].value_counts().to_dict())\n",
    "print(\"Sub counts :\", df_all[\"sub_name\"].value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "631047b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/val/test sizes: 1280 160 161\n",
      "train sub dist: {'Littering / Garbage': 666, 'Pothole': 514, 'Illegal Parking': 82, 'Damaged Road Surface': 18}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from pathlib import Path\n",
    "\n",
    "CSV  = ROOT / \"labels\" / \"dataset_main_sub.csv\"\n",
    "SPL = ROOT / \"labels\" / \"splits_hier\"; SPL.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(CSV)\n",
    "\n",
    "# keep only rows with a single sub id (already enforced), and valid ids\n",
    "df = df[df[\"sub_id\"].notna()].copy()\n",
    "\n",
    "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n",
    "tr_idx, tmp_idx = next(sss1.split(df[\"path\"].values, df[\"sub_id\"].values))\n",
    "df_tr, df_tmp = df.iloc[tr_idx], df.iloc[tmp_idx]\n",
    "\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.50, random_state=43)  # 10/10\n",
    "v_idx, te_idx = next(sss2.split(df_tmp[\"path\"].values, df_tmp[\"sub_id\"].values))\n",
    "df_val, df_test = df_tmp.iloc[v_idx], df_tmp.iloc[te_idx]\n",
    "\n",
    "df_tr.to_csv(SPL/\"train.csv\", index=False)\n",
    "df_val.to_csv(SPL/\"val.csv\",   index=False)\n",
    "df_test.to_csv(SPL/\"test.csv\", index=False)\n",
    "\n",
    "print(\"train/val/test sizes:\", len(df_tr), len(df_val), len(df_test))\n",
    "print(\"train sub dist:\", df_tr[\"sub_name\"].value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9c65d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classifyvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
